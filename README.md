# face_recognition — FastAPI backend

**Simple, minimal FastAPI service** for a face recognition web application.  
Provides tools to build serialized face encodings from a labeled image dataset (`train.pickle`), run a FastAPI server for inference, confirm labels, and reset training data.

---

## Purpose / How it works
This repository implements the **backend** for a face-recognition application.  
When a user uploads a photo the service:

1. **Checks** whether the person is already known (i.e., at least one labeled image exists).  
2. If the person is known, the API **returns the predicted identity** and a confidence score.  
3. If the person is **not** known, the app allows an operator or user to **confirm and add a label** for that person (via `POST /confirm/`). The new labeled encoding is appended to the training data so future uploads of the same person will be recognized correctly.

This flow enables the system to learn incrementally: a person only needs to be labeled once and subsequent uploads will be labeled automatically.

---

## Features
- Build a single `train.pickle` containing all face encodings + a `LabelEncoder`.  
- FastAPI endpoints to **upload & predict**, **confirm** a label, **reset** training data, and a health check.  
- Uses PCA + Label Propagation for inference; detection can use `cnn` (default, more accurate) or `hog`.

---

## Requirements
- **Python 3.9** (the repo includes `backend/environment.yml`).  
- Key libraries: `face-recognition` (dlib), `opencv-python`, `scikit-learn`, `fastapi`, `uvicorn`, `numpy`.  
> Note: `dlib` and `face-recognition` commonly require system build tools (or conda) — use the provided `environment.yml` or conda where possible.

---

## Quick start (conda)
```bash

git clone https://github.com/TayebKahia/face_recognition.git
cd face_recognition/backend
conda env create -f environment.yml
conda activate face_recog
````

*(Or install the packages from `environment.yml` with `pip` if you prefer — but `dlib/face-recognition` may need extra system deps.)*

---

## Prepare dataset & create encodings

Create these directories at the repo root :

```
dataset/             # temporary uploads
dataset_training/    # labeled images for training
dataset_reserve/     # reserve images used by reset
```

From `backend/` generate `train.pickle`:

```bash
python create_training_encodings.py -d ../dataset_training -o ../train.pickle -m cnn
```

* `-d` : labeled image directory
* `-o` : output pickle path (default `train.pickle`)
* `-m` : detection method: `cnn` (default) or `hog`

`train.pickle` format: a dict with `"encodings"` (list of `{imagePath, loc, encoding, label}`) and `"label_encoder"` (`{"classes_": [...]}`).

---

## Run the server

From the repo root (or `backend/`):

```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

---

## API (quick)

* `POST /upload/` — upload `multipart/form-data` field `file`; returns `predicted_label`, `confidence_score`, `image_path`.

  * If no face is found or no confident match exists, the response indicates this and the upload can be confirmed manually.
* `POST /confirm/` — confirm a predicted label; form fields `image_path` (e.g. `dataset/photo.jpg`) and `label` (string). Confirms, appends encoding to `train.pickle`, and moves the image into `dataset_training/`.
* `POST /reset_training/` — clears `dataset/` & `dataset_training/`, copies images from `dataset_reserve/` to `dataset_training/`, and regenerates `train.pickle`.
* `GET /` — health check (`{"hello":"world"}`).

---

## Notes & suggestions

* The repo is intended as a minimal backend for experiments. For production, add: unit tests, robust error handling, a database or atomic artifact storage instead of a single pickle, and a Dockerfile/docker-compose for reproducible deployment.
* Consider including a tiny sample dataset + example `train.pickle` for quick local testing.

---

## Project structure (important files)

```
backend/
├─ main.py                      # FastAPI app + endpoints
├─ create_training_encodings.py # build train.pickle
├─ environment.yml              # conda env
dataset/
dataset_training/
dataset_reserve/
train.pickle                    # generated by create_training_encodings.py
```

