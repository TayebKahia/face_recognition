
## 1) `README.md` (paste at repo root)

```markdown
# face_recognition (FastAPI Backend)

**Simple face recognition backend** using `face_recognition` (dlib-based encodings), OpenCV, scikit-learn, and FastAPI.  
This service provides tools to create/serialize face encodings from a labeled dataset, run a FastAPI server for inference, confirm labels, and reset training data.

---

## Features
- Create and serialize facial encodings from a labeled image dataset into a single `train.pickle` file (encodings + label encoder).  
- FastAPI endpoints to:
  - Upload an image and predict the person using PCA + Label Propagation.  
  - Confirm a predicted label and update the training dataset.  
  - Reset training data (copy reserve images into training and regenerate encodings).  
- Minimal, easy-to-extend codebase for experimenting with face recognition workflows.

---

## Project structure (important files)
```

backend/
├─ main.py                      # FastAPI app + endpoints
├─ create_training_encodings.py # Script to create train.pickle from dataset
└─ environment.yml              # Conda environment with dependencies
dataset/                          # (used by API for uploads)
dataset_training/                 # labeled training images
dataset_reserve/                  # reserve images used by reset
train.pickle                      # serialized encodings (generated by create_training_encodings.py)

````

> The main logic is in `backend/main.py` (endpoints, PCA & Label Propagation inference) and `backend/create_training_encodings.py` (builds `train.pickle`). See those files for implementation details. :contentReference[oaicite:3]{index=3} :contentReference[oaicite:4]{index=4}

---

## Requirements & notes
- Designed for **Python 3.9** (see `backend/environment.yml` for full dependencies). Key packages:
  - `fastapi`, `uvicorn`
  - `face-recognition`, `dlib`
  - `opencv`, `scikit-learn`, `numpy`, `imutils`, `python-dotenv`  
  (Some packages such as `dlib` and `face-recognition` often require system-level build tools — see their install docs.) :contentReference[oaicite:5]{index=5}

---

## Setup (Conda)
1. Clone the repository:
```bash
git clone https://github.com/<your-username>/face_recognition.git
cd face_recognition/backend
````

2. Create & activate the Conda environment:

```bash
conda env create -f environment.yml
conda activate face_recog
```

> If you prefer `pip`, install the packages listed in `environment.yml`. Installing `dlib` and `face-recognition` may require `cmake` and build dependencies.

---

## Prepare your dataset

* Create these directories in the repo root if they do not exist:

```
dataset/             # temporary uploads
dataset_training/    # labeled images for training (input to create_training_encodings.py)
dataset_reserve/     # images to restore training dataset on reset
```

* The **label convention** used by the encoding script: the label is taken from the filename (e.g., `alice_1.jpg` → `alice_1`). For person-level labels use filenames like `alice_01.jpg`, `alice_02.jpg` etc. 

---

## Create training encodings (`train.pickle`)

From `backend/` run:

```bash
python create_training_encodings.py -d ../dataset_training -o ../train.pickle -m cnn
```

* `-d` : directory of labeled images
* `-o` : output pickle path (default `train.pickle`)
* `-m` : detection method: `cnn` (default) or `hog`

The script outputs a single pickle file containing:

```json
{
  "encodings": [
    { "imagePath": "...", "loc": [...], "encoding": [...], "label": int }, ...
  ],
  "label_encoder": { "classes_": ["label1","label2", ...] }
}
```

This file is read by the API at runtime. 

---

## Run the FastAPI server

From repo root (or `backend/`):

```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

* Application variable: `app` in `backend/main.py`. 

---

## API Endpoints & Examples

### `POST /upload/`

Upload an image to get a predicted label. Accepts `multipart/form-data` with field `file`.

**Example (curl):**

```bash
curl -X POST "http://localhost:8000/upload/" \
  -F "file=@/path/to/photo.jpg"
```

**Response (example):**

```json
{
  "predicted_label": "alice",
  "confidence_score": 0.82,
  "image_path": "dataset/photo.jpg"
}
```

**What happens:**

* The image is saved to `dataset/`.
* The server loads `train.pickle`, extracts encoded vectors and labels, appends the uploaded encoding, runs PCA + Label Propagation, and returns the predicted label and confidence. If no face is found the endpoint returns an error. 

---

### `POST /confirm/`

Confirm a label for an uploaded image. Expects form fields:

* `image_path` (string): e.g., `dataset/photo.jpg`
* `label` (string): confirmed label (e.g., `alice`)

**Example (curl):**

```bash
curl -X POST "http://localhost:8000/confirm/" \
  -F "image_path=dataset/photo.jpg" \
  -F "label=alice"
```

**What it does:**

* Encodes the image, appends it to the `encodings` list in `train.pickle` (after mapping label to an integer using `LabelEncoder`), and moves the image to `dataset_training/`. Filename collisions are handled automatically. 

---

### `POST /reset_training/`

Reset the training dataset:

* Deletes all files in `dataset/` and `dataset_training/`,
* Copies images from `dataset_reserve/` into `dataset_training/`,
* Runs `create_training_encodings.py` to regenerate `train.pickle`.

**Example (curl):**

```bash
curl -X POST "http://localhost:8000/reset_training/"
```

This endpoint calls the encode script automatically to regenerate `train.pickle`. 

---

### `GET /`

Health endpoint:

```json
{"hello":"world"}
```

---

## Data format & implementation notes

* `train.pickle` stores `encodings` as a list of dicts: `{imagePath, loc, encoding, label}` and a `label_encoder` mapping as `{"classes_": [...]}`. This exact format is required by the API. 
* Face detection defaults to `cnn` for better accuracy but is slower than `hog`. You can change detection via `-m` when generating encodings. 

---

## Limitations & suggestions

* The repo assumes single-face or expects behavior based on how encodings are combined; multiple-face uploads may need extra handling. 
* Consider adding:

  * A small example dataset and a sample `train.pickle` for quick testing.
  * A Dockerfile or `docker-compose.yml` for reproducible deployment.
  * Unit tests and better error handling for production use.

---

## Where to look in the code

* `backend/main.py` — FastAPI server, upload/confirm/reset endpoints, PCA & Label Propagation logic. 
* `backend/create_training_encodings.py` — builds `train.pickle` from labeled images. 
* `backend/environment.yml` — Conda dependencies. 

---

## License

No license file is included. Add `LICENSE` (e.g., MIT) if you want permissive reuse.

````

---

## 2) `backend/README.md` (paste into `backend/README.md`)

```markdown
# Backend — face_recognition

This folder contains the FastAPI backend and the training encoding script used by the face recognition project.

## Files
- `main.py` — FastAPI application:
  - Endpoints:
    - `POST /upload/` : accept an image, compute encoding, run PCA + Label Propagation for prediction.
    - `POST /confirm/`: confirm predicted label and append to training data.
    - `POST /reset_training/`: reset training dataset and regenerate `train.pickle`.
    - `GET /` : basic health check.
  - Uses constants at top: `DATASET_FOLDER`, `DATASET_TRAINING_FOLDER`, `TRAINING_DATASET_PATH`, `DETECTION_METHOD`. Edit these as needed. :contentReference[oaicite:18]{index=18}

- `create_training_encodings.py` — script that:
  - Scans a labeled image directory, extracts faces & encodings, maps string labels to integers with `LabelEncoder`, and serializes a combined dictionary (encodings + label encoder) into a single pickle file (`train.pickle`). Usage example is shown below. :contentReference[oaicite:19]{index=19}

- `environment.yml` — conda environment file with core dependencies (Python 3.9, FastAPI, face-recognition, dlib, scikit-learn, OpenCV, etc.). Use to reproduce the environment. :contentReference[oaicite:20]{index=20}

---

## Quick commands

- Create encodings:
```bash
python create_training_encodings.py -d ../dataset_training -o ../train.pickle -m cnn
````

* Run server (from repo root):

```bash
uvicorn backend.main:app --reload --host 0.0.0.0 --port 8000
```

---

## Tips for development & production

* **Environment:** `dlib` / `face-recognition` may need system packages. Prefer `conda` where possible or use pre-built wheels.
* **Storage:** `train.pickle` is an easy serialized format for experiments. For production consider a database or more robust model/artifact storage with atomic updates.
* **Refactor idea:** `main.py` currently embeds logic for PCA + Label Propagation, saving, and file moves. Consider splitting concerns into modules:

  * `utils/encoding.py` — encode & decode utilities
  * `services/training.py` — training dataset and serialization
  * `api/routes.py` — endpoints only
* **Testing:** add sample images and a small `train.pickle` for automated tests and local experimentation.

---

## Contact / next steps

* Add example images (`dataset_reserve`) and commit a small `train.pickle` with a few labels to enable quick sanity checks.
* Add a `Dockerfile` to simplify running the backend on any machine.

```

---

### Final notes
- Paste the **first block** into the repository root as `README.md`. Paste the **second block** into `backend/README.md`.  
- If you want, I can also:
  - Create a `Dockerfile` and `docker-compose.yml` for the backend, or  
  - Create a `train.pickle` example + a small sample dataset with 2–3 fake images for quick testing, or  
  - Commit these files to the repository (if you authorize and provide access).

Would you like me to produce a `Dockerfile` next, or add sample images / a sample `train.pickle`?
```
